{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "CLIP wrapper takes batched tensors or text queries and returns batched 512-dim vectors. size of batch depends on GPU, but if we're putting all that on a server anyway it's a matter of accounting. Does batching go here though? Or in the crafter?\n",
    "\n",
    "cool thing here is we can use one encoder for both image and text, just check type on the way in. but first probably keep it simple and make two functions.\n",
    "\n",
    "could index previous queries as vectors in a different map and use for predictive/history -- keep a little database of previous queries already in vector format and their ranked NNs, so that the user can see history offline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, _ = clip.load(\"ViT-B/32\", device)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def image_encoder(img_loader, device):\n",
    "    image_embeddings = torch.tensor(()).to(device)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(img_loader):\n",
    "            batch_features = model.encode_image(images)\n",
    "            image_embeddings = torch.cat((image_embeddings, batch_features)).to(device)\n",
    "    \n",
    "    image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "    return(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def text_encoder(text, device):\n",
    "    with torch.no_grad():\n",
    "        text = clip.tokenize(text).to(device)\n",
    "        text_features = model.encode_text(text)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    return(text_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
