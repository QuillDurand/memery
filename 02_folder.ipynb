{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MemeFolder\n",
    "\n",
    "> A convenient wrapper around CLIP. Takes a folder of images and a query and returns a ranked list of image filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torchvision\n",
    "import clip\n",
    "\n",
    "from pathlib import Path\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MemeFolder:\n",
    "    \"\"\"Takes an image folder and a CLIP model and calculates the encodings for each image\"\"\"\n",
    "    \n",
    "    def __init__(self, folder_str, clip_model=\"ViT-B/32\", clear_cache=False, use_treemap=True):\n",
    "        self.clear_cache = clear_cache\n",
    "        self.path = Path(folder_str)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model, self.preprocess = clip.load(clip_model, device=self.device)\n",
    "        self.logit_scale = self.model.logit_scale.exp()\n",
    "\n",
    "        if use_treemap == True:\n",
    "            self.names = self.images_to_treemap(self.preprocess)\n",
    "        else: \n",
    "            self.names, self.features = self.images_to_dict()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return(f'MemeFolder from {str(self.path)}, {len(self.names)} images')\n",
    "    \n",
    "    def __repr__(self): return(self.__str__())\n",
    "    \n",
    "    def __len__(self): return(len(self.names))\n",
    "    \n",
    "    def preproc_images(self):\n",
    "        '''Batch process images with CLIP and return their feature embeddings'''\n",
    "        image_features = torch.tensor(()).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            imagefiles=torchvision.datasets.ImageFolder(root=self.path, transform=self.preprocess)\n",
    "            img_loader=torch.utils.data.DataLoader(imagefiles, batch_size=128, shuffle=False, num_workers=4)\n",
    "            for images, labels in tqdm(img_loader):\n",
    "                batch_features = self.model.encode_image(images)\n",
    "                image_features = torch.cat((image_features, batch_features)).to(self.device)\n",
    "\n",
    "        image_names = [Path(f[0]) for f in imagefiles.imgs]\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return(image_names, image_features)\n",
    "        \n",
    "    def images_to_dict(self):\n",
    "        \"\"\"Calculate image encodings from folder. \n",
    "        TODO: Fix recursive loading to include self folder\n",
    "        \"\"\"\n",
    "        savefile = self.path/'memery.pt'\n",
    "        if self.clear_cache == True and savefile.exists():\n",
    "            savefile.unlink() # remove savefile if need be\n",
    "        # load or generate the encodings üóúÔ∏è\n",
    "        # currently this just checks to see if there's a savefile, not if anything has changed since save time\n",
    "        if savefile.exists():\n",
    "            save_dict = torch.load(savefile)\n",
    "            image_names = [k for k in save_dict.keys()]\n",
    "            image_features = torch.stack([v for v in save_dict.values()]).to(self.device)\n",
    "        else:\n",
    "            image_names, image_features = self.preproc_images()\n",
    "            save_dict = {str(k):v for k, v in zip(image_names, image_features)}\n",
    "            torch.save(save_dict, savefile)                           \n",
    "        return(image_names, image_features)\n",
    "\n",
    "    def images_to_treemap(self, preprocess, clear_cache=False):\n",
    "        \"\"\"Calculate image encodings from folder and encode to treeman\n",
    "        TODO: Fix recursive loading to include self folder, incorporate with images_to_dict better\n",
    "        \"\"\"\n",
    "        self.treemap = AnnoyIndex(512, 'angular')\n",
    "        savefile = self.path/'memery.ann'\n",
    "        namefile = self.path/'names.txt'\n",
    "\n",
    "        if clear_cache == True and savefile.exists():\n",
    "            savefile.unlink()\n",
    "\n",
    "        if savefile.exists():\n",
    "            self.treemap.load(str(savefile))\n",
    "            with open(str(namefile), 'r') as f:\n",
    "                image_names =  [Path(o[:-1]) for o in f.readlines()]\n",
    "        else:\n",
    "            image_names, image_features = self.preproc_images()\n",
    "            print(\"building trees...\")\n",
    "            for i, img in enumerate(image_features):\n",
    "                self.treemap.add_item(i, img)\n",
    "                \n",
    "            # Build the treemap, with 10 trees rn\n",
    "            self.treemap.build(10)\n",
    "\n",
    "            # Save annoy index and list of filenames\n",
    "            self.treemap.save(str(savefile))    \n",
    "            with open(str(namefile), 'w') as f:\n",
    "                f.writelines([f'{str(o)}\\n' for o in image_names])\n",
    "        return(image_names)\n",
    "\n",
    "    def predict_from_text_dict(self, query):\n",
    "        \"\"\"CLIPify the text query and compare to each image. Returns a sorted dictionary of names\n",
    "        and scores\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            text = clip.tokenize(query).to(self.device)\n",
    "            text_features = self.model.encode_text(text)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # matrix-vector product as logits\n",
    "            logits_per_image = self.logit_scale * self.features @ text_features.float().t()\n",
    "\n",
    "        scores = {self.names[i]: logit for i, logit in enumerate(logits_per_image)}\n",
    "        top_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        return([str(file) for file, score in top_scores])\n",
    "    \n",
    "    def predict_from_text_trees(self, query):\n",
    "        \"\"\"CLIPify the text query and find nearest neighbors using treemap\"\"\"\n",
    "        with torch.no_grad():\n",
    "            text = clip.tokenize(query).to(self.device)\n",
    "            text_features = self.model.encode_text(text)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        nn_indexes = self.treemap.get_nns_by_vector(text_features.t(), self.treemap.get_n_items())\n",
    "        return([str(self.names[i]) for i in nn_indexes])\n",
    "\n",
    "#     def predict_from_image_trees(self, query_img):\n",
    "#         \"\"\"UNTESTED: CLIPify an image and find nearest neighbors using treemap. doesn't work yet\"\"\"\n",
    "#         with torch.no_grad():\n",
    "#             image = clip.tokenize(query_images).to(device)\n",
    "#             image_features = model.encode_image(image)\n",
    "#             image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#         nn_indexes = t.get_nns_by_vector(image_features.t(), self.treemap.get_n_items())\n",
    "#         return(nn_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MemeFolder is a class that represents a folder of images, and allows you to search through them by text or by image. The semantic search is powered by OpenAI's CLIP model. The MemeFolder class provides methods for inference and for storing and retrieving embeddings.\n",
    "\n",
    "For example, here's one made from the small set of memes in the local `images` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = MemeFolder('/home/mage/Pictures/reactjpg/', use_treemap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9899"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can predict text from a `MemeFolder`'s methods. For smaller datasets, the simpler method `predict_from_text_trees` can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = small.predict_from_text_dict('cat')\n",
    "cats[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check those images using a quick and dirty `printi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printi(images, n = 3, w = 200, start_index = 0):\n",
    "    for im in images[start_index:start_index + n]:\n",
    "#         print(f'{im}')\n",
    "        try:\n",
    "            display(Image(filename=im, width=w))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "# printi(image_names, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printi(cats, 3, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weirdly, there's not many cat memes in the sample dataset. Let's try with man's other best friend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs = small.predict_from_text_dict('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printi(dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dogs) == len(small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better! Now, if your dataset is more than ~1000 images the `predict_text_from_trees` method will work much faster. Right now the `MemeFolder` generates a treemap by default. This will slow down the initiation of the folder, but massively speed up inference. \n",
    "\n",
    "Generating the treemap may be a minor slowdown for smaller datasets. You can prevent it by passing `use_treemap = False` when creating the `MemeFolder`. For most purposes the treemap is better though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = MemeFolder('images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doggos = small.predict_from_text_trees('dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printi(doggos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The really amazing thing about CLIP is that it does semantic search and text recognition at the same time. So not only can it recognize an image of a dog, it can also see a meme template with the word \"dog\" above it! \n",
    "\n",
    "For a more dramatic example, let's search the dataset for \"penis\" (and hope dearly that there aren't any dick pics in here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penii = small.predict_from_text_trees('penis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printi(penii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though `printi` only shows the top 3 by default, the MemeFolder returns all of the images in ranked order. This can be useful for, e.g., finding the least \"penis\" images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_penii = sorted(penii, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printi(not_penii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enlightening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "For some reason the treemap method is causing the length of the folder to be shortened. Why is this happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doggos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up a dataset, delete later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_images(imagelist):\n",
    "    for im in imagelist:\n",
    "        if Path(im).exists():\n",
    "            Path(im).unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles = MemeFolder('/home/mage/Hacking/datasets/circles-rgb-proc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurs = circles.predict_from_text_trees('a blurry image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printi(blurs, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_images(blurs[:12])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
