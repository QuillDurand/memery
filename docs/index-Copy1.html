---

title: memery


keywords: fastai
sidebar: home_sidebar

summary: "Search over large image datasets with natural language and computer vision!"
description: "Search over large image datasets with natural language and computer vision!"
nb_path: "index-Copy1.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: index-Copy1.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>^ that is so nerdy. can we do something like "Local-first AI image search"? or is that still too opaque</em></p>
<p><img src="/memery/images/E2GoeMyWEAAkcLz.jpeg" alt="meme about having too many memes"></p>
<p>The problem: you have a huge folder of images. Memes, screenshots, datasets, product photos, inspo albums, anything. You know that somewhere in that folder is the exact image you want, but you can't remember the filename or what day you saved it. There's nothing you can do buit scroll through the folder, skimming hundreds of thumbnails, hoping you don't accidentally miss it, hoping you'll recognize it when you do see it.</p>
<p>Humans do this amazingly well. But even with computers, local image search is still a manual effort - you're still sorting through folders of images, like an archivist of old.</p>
<p><strong>Until memery</strong>.</p>
<p>The <code>memery</code> package provides natural language search over local images. You can use it to search for things like "a line drawing of a woman facing to the left" and get <em>reasonably good results!</em></p>
<p>You can do this over thousands of images (it's not optimized for performance yet, but search times scale well under O(n)).</p>
<p>You can view the images in a browser GUI, or pipe them through command line tools.</p>
<p>You can use <code>memery</code> or its modules in Jupyter notebooks, including GUI functions!</p>
<p>Under the hood, <code>memery</code> makes use of <strong>CLIP</strong>, the <a href="https://github.com/openai/CLIP">Contrastive Language-Image Pretraining transformer</a>, released by OpenAI in 2021. CLIP trains a vision transformer and a language transformer to find the same latent space for images and their captions. This makes it perfect for the purpose of natural language image search. CLIP is a giant achievement, and <code>memery</code> stands on its shoulders.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Outline:</p>
<ul>
<li>Usage<ul>
<li>Install locally</li>
<li>Use GUI</li>
<li>Use CLI</li>
<li>Use in Jupyter</li>
<li>Use the library</li>
</ul>
</li>
<li>Development<ul>
<li>Notebook-driven development</li>
<li>Pull the repo</li>
<li>Branch and install</li>
<li>Notebook-driven development</li>
<li>Change the notebooks</li>
<li>Test the notebooks</li>
<li>Notebook-driven development</li>
<li>Tangle the source code</li>
<li>Weave the documentation</li>
</ul>
</li>
<li>Contributing<ul>
<li>Who works on this project</li>
<li>How you can help</li>
<li>What we don't do</li>
<li>Thanks</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Install">Install<a class="anchor-link" href="#Install"> </a></h2><p>The necessary CLIP and torch packages will be installed by pip.</p>
<p><em>what does this mean? why are you telling me this? what if they aren't?</em></p>
<p>You might want to make sure you have a sane CUDA environment before and after this step if you're trying to use GPU. If you don't have a GPU, <code>memery</code> should still work on your CPU.</p>
<p><em>this is backwards and also impenetrable if you're not a nerd</em></p>
<p>If you have any trouble please <strong>open an issue on Github</strong>! I want to make this package useful for as many people as possible. Help me know what's going wrong :)</p>
<p><em>still true, pretty clear. use emoji or go home though</em></p>
<p><em>This is the crucial command, huh? Seems a little far down the page. Maybe a quick-start section earlier, then a more in-depth Installation section here</em></p>
<p><code>pip install memery</code></p>
<p>If you don't have a GPU for PyTorch, this command might help:
<code>pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html</code></p>
<p><em>did you scrape this from stackoverflow? or the CLIP source code or what. it looks very specific and arbitrary. I think it's meant to install pytorch CPU version, but the average user shouldn't have to know that! if you are trying to use CUDA and GPU it should be an easy toggle, and if you're a regular person wihtout a data science project you shouldn't have to go through a bunch of weird command line stuff.</em></p>
<p><em>In fact, how can we package this in a way that you don't have to type a single command? There has to be ways to wrap up a whole python setup and install it on a different computer, with all the same dependencies. Right? Ugh, unfortunately this is Python, so it might not exist. But I can at least scale the difficulty level with the user story.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Yes, user stories. We have to think about the interface between human and computer as another API, and look for the endpoints we can serve to different users with different protocols.</em></p>
<p><em>This means thinking about the reasons people come to the program, and how they expect it to respond. Currently I see three user stories:</em></p>
<ul>
<li><em>i have images and want to search them with a GUI app</em><ul>
<li><em>streamlit GUI</em></li>
</ul>
</li>
<li><em>i have a program/workflow and want to use image search as one part of it</em><ul>
<li><em>CLI tool</em></li>
<li><em>python module</em></li>
<li><em>jupyter GUI</em></li>
</ul>
</li>
<li><em>i want to improve on and/or contribute to memery development</em><ul>
<li><em>clone the repo</em></li>
</ul>
</li>
</ul>
<p><em>Each one has an increasing level of complexity and expected knowledge.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-to-use">How to use<a class="anchor-link" href="#How-to-use"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Use-GUI">Use GUI<a class="anchor-link" href="#Use-GUI"> </a></h3><p>The browser GUI is a Streamlit app. You can run it from the command line with</p>
<p><code>memery serve</code></p>
<p>or set up a desktop shortcut to use it from your menu.</p>
<p><em>Why do I have to use a CLI to get a GUI? This is very annoying. In any case, this does not provide instructuions on how to use the GUI or any of its particular quirks. There should be a screenshot, also</em></p>
<p>If you're in a Jupyter environment, you can summon an ipywidgets GUI directly into an output cell like this:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">memery.gui</span> <span class="kn">import</span> <span class="n">appPage</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">app</span> <span class="o">=</span> <span class="n">appPage</span><span class="p">()</span>
<span class="n">display</span><span class="p">(</span><span class="n">app</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea ">
<pre>&lt;memery.gui.appPage at 0x7ff3072db110&gt;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>This doesn't transfer to the README, so it should be fixed or be scrapped. Screenshots would work fine. Also I'm not sure if the interactive widgets would make sense without a kernel behind them anyway.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Use-CLI">Use CLI<a class="anchor-link" href="#Use-CLI"> </a></h3><p>From the command line, you can use <code>memery</code> on any folder and it will search for images recursively, returning a list object to stdout.</p>
<p><em>Why is it that behavior? Is it possible to make it not recursive? What is the usual behavior of a search in POSIX? What is the use case for a CLI memery: is it shell scripts? in-console image browsing? risk-tolerant batch modification? Think about this further</em></p>
<p><em>at least you can control how much output it spews to your terminal</em></p>
<p>Pass the --n flag to control how many images are returned (default 10).</p>
<p><code>memery recall PATH/TO/IMAGE/FOLDER 'query' --n 20</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Use-as-a-library">Use as a library<a class="anchor-link" href="#Use-as-a-library"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>the following paragrah cannot be comprehended by any human mind. if you can even read to the end of it you should go touch grass</em></p>
<p>Simply use <a href="/memery/core.html#queryFlow"><code>queryFlow</code></a> to search over a folder recursively! The folder will be indexed, if an index doesn't already exist. Then any new images will be CLIP-encoded, an Annoy treemap built, and a list of ranked filenames returned.</p>
<p><em>okay, we survived. What are we supposed to say here instead?</em></p>
<p><em>I think what he's trying to get at is explaining the underlying mechanism of the search. Really all we need here is a function signature, as anyone who's going to use it as a library will probably also be looking at the docs for further information. Have a link here too, that goes to the <a href="/memery/core.html"><code>core</code></a> file where the main flows and executors are explained.</em></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">memery.core</span> <span class="kn">import</span> <span class="n">queryFlow</span>
<span class="kn">from</span> <span class="nn">memery.gui</span> <span class="kn">import</span> <span class="n">get_grid</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ranked</span> <span class="o">=</span> <span class="n">queryFlow</span><span class="p">(</span><span class="s1">&#39;./images&#39;</span><span class="p">,</span> <span class="s1">&#39;dad joke&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ranked</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p>*Okay, now we need a whole part about development.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><em>Compile this notebook</em></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">nbdev.export</span> <span class="kn">import</span> <span class="n">notebook2script</span><span class="p">;</span> <span class="n">notebook2script</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Converted 00_core.ipynb.
Converted 01_loader.ipynb.
Converted 02_crafter.ipynb.
Converted 03_encoder.ipynb.
Converted 04_indexer.ipynb.
Converted 05_ranker.ipynb.
Converted 06_fileutils.ipynb.
Converted 07_cli.ipynb.
Converted 08_jupyter_gui.ipynb.
Converted 09_streamlit_app.ipynb.
Converted index.ipynb.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

<script type="application/vnd.jupyter.widget-state+json">
{"state": {}, "version_major": 2, "version_minor": 0}
</script>

