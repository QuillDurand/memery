{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "Find the meme you are looking for!\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first iteration of `memery` is a simple CLI tool that you can use on a folder with subfolders of images to return the closest `n` images based on a text or image search. \n",
    "\n",
    "This is the core module, which loads the CLIP model and the encodings of all the images in the folder, then tokenizes the search text or image, and finally returns a sorted list of filenames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CLIP here. There are 4 pretrained models available: `RN50`, `RN101`, `RN50x4` and `ViT-B/32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import clip\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from IPython.display import Image, display\n",
    "from PIL import Image as Img, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set torch üî¶ device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# load CLIP üìé model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device)\n",
    "logit_scale = model.logit_scale.exp()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_images(path, preprocess, clear_cache=False):\n",
    "    savefile = path/'memery.pt'\n",
    "    if clear_cache == True:\n",
    "        savefile.unlink() # remove savefaile if need be\n",
    "    # load or generate the encodings üóúÔ∏è\n",
    "    # currently this just checks to see if there's a savefile, not if anything has changed since save time\n",
    "    if savefile.exists():\n",
    "        save_dict = torch.load(savefile)\n",
    "        image_names = [k for k in save_dict.keys()]\n",
    "        image_features = torch.stack([v for v in save_dict.values()]).to(device)\n",
    "    else:\n",
    "        image_features = torch.tensor(()).to(device)\n",
    "        with torch.no_grad():\n",
    "            imagefiles=torchvision.datasets.ImageFolder(root=path, transform=preprocess)\n",
    "            img_loader=torch.utils.data.DataLoader(imagefiles, batch_size=128, shuffle=False, num_workers=4)\n",
    "            for images, labels in tqdm(img_loader):\n",
    "                batch_features = model.encode_image(images)\n",
    "                image_features = torch.cat((image_features, batch_features)).to(device)\n",
    "\n",
    "        image_names = [Path(f[0]) for f in imagefiles.imgs]\n",
    "\n",
    "        save_dict = {str(k):v for k, v in zip(image_names, image_features)}\n",
    "        torch.save(save_dict, savefile)\n",
    "\n",
    "    return(image_names, image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printi(images, n = 3, w = 200, start_index = 0):\n",
    "    for im in images[start_index:start_index + n]:\n",
    "        print(f'{im}')\n",
    "        try:\n",
    "            display(Image(filename=im, width=w))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "# printi(image_names, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92, torch.Size([92, 512]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the path üåÑ\n",
    "# path = Path('/home/mage/Pictures/occult-imagery')\n",
    "path = Path('./images')\n",
    "image_names, image_features = collate_images(path, preprocess, clear_cache=False)\n",
    "len(image_names), image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dict = {str(k):v for k, v in zip(image_names, image_features)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search and rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a string and compare its encoding to the encoding of each image. Return a sorted list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_text(image_names, image_features, query):\n",
    "    with torch.no_grad():\n",
    "        text = clip.tokenize(query).to(device)\n",
    "        text_features = model.encode_text(text)\n",
    "\n",
    "        # normalize features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # matrix-vector product as logits\n",
    "        logits_per_image = logit_scale * image_features @ text_features.float().t()\n",
    "\n",
    "    # make sure the shapes make sense\n",
    "    # print(logits_per_image.shape, all_image_features.shape, text_features.shape)\n",
    "\n",
    "    scores = {image_names[i]: logit for i, logit in enumerate(logits_per_image)}\n",
    "    top_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    return(top_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''A picture of a dog'''\n",
    "results = predict_from_text(image_names, image_features, query)\n",
    "# inv_results = sorted(results, key=lambda o: o[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
